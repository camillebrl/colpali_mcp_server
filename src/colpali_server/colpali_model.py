"""ModÃ¨le ColPali pour la gÃ©nÃ©ration d'embeddings d'images et de requÃªtes."""

import gc
import logging
import time
from collections.abc import Sequence
from threading import Lock
from typing import Any, Optional

import torch
from PIL import Image
from transformers.utils.import_utils import is_flash_attn_2_available

try:
    from colpali_engine.models import ColQwen2, ColQwen2Processor
except ImportError:
    ColQwen2 = None
    ColQwen2Processor = None


logger = logging.getLogger(__name__)


class ColPaliModelManager:
    """Gestionnaire singleton pour le modÃ¨le ColPali avec chargement/dÃ©chargement dynamique."""

    _instance: Optional["ColPaliModelManager"] = None
    _lock = Lock()

    def __new__(cls, *args: Any, **kwargs: Any) -> "ColPaliModelManager":
        """CrÃ©e ou retourne l'instance singleton du gestionnaire ColPali.

        Args:
            *args: Arguments positionnels (ignorÃ©s).
            **kwargs: Arguments nommÃ©s (ignorÃ©s).

        Returns:
            ColPaliModelManager: L'instance unique du gestionnaire.
        """
        if not cls._instance:
            with cls._lock:
                if not cls._instance:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self) -> None:
        """Initialise le gestionnaire ColPali (une seule fois pour le singleton)."""
        # Ne rÃ©initialiser que si c'est la premiÃ¨re fois
        if not hasattr(self, "_initialized"):
            self._initialized = True
            self._model: ColQwen2 | None = None
            self._processor: ColQwen2Processor | None = None
            self._model_path: str | None = None
            self._last_used: float = 0
            self._load_lock = Lock()
            self._reference_count = 0
            self._unload_delay = 0  # DÃ©lai en secondes avant dÃ©chargement

    def acquire(self, model_path: str = "vidore/colqwen2-v1.0") -> "ColPaliModel":
        """Acquiert une rÃ©fÃ©rence au modÃ¨le et le charge si nÃ©cessaire.

        Args:
            model_path (str): Chemin vers le modÃ¨le ColPali Ã  charger.

        Returns:
            ColPaliModel: Instance du modÃ¨le prÃªte Ã  utiliser.
        """
        with self._load_lock:
            self._reference_count += 1
            self._last_used = time.time()

            # Charger le modÃ¨le si pas dÃ©jÃ  chargÃ© ou si le chemin a changÃ©
            if self._model is None or self._model_path != model_path:
                self._load_model(model_path)

            return ColPaliModel(self)

    def release(self) -> None:
        """LibÃ¨re une rÃ©fÃ©rence au modÃ¨le."""
        with self._load_lock:
            self._reference_count = max(0, self._reference_count - 1)
            self._last_used = time.time()

            # Ne pas dÃ©charger immÃ©diatement si d'autres rÃ©fÃ©rences existent
            if self._reference_count == 0:
                self._unload_model()
                return

            # Programmer le dÃ©chargement aprÃ¨s le dÃ©lai
            # Note: Dans une vraie implÃ©mentation, on utiliserait un timer thread
            # Pour simplifier, on laisse la responsabilitÃ© Ã  check_and_unload()

    def check_and_unload(self) -> None:
        """VÃ©rifie si le modÃ¨le doit Ãªtre dÃ©chargÃ© (appelÃ© pÃ©riodiquement)."""
        with self._load_lock:
            if (
                self._model is not None
                and self._reference_count == 0
                and time.time() - self._last_used > self._unload_delay
            ):
                self._unload_model()

    def _load_model(self, model_path: str) -> None:
        """Charge le modÃ¨le en mÃ©moire.

        Args:
            model_path (str): Chemin vers le modÃ¨le Ã  charger.

        Raises:
            ImportError: Si colpali_engine n'est pas disponible.
            Exception: Si le chargement Ã©choue.
        """
        if ColQwen2 is None or ColQwen2Processor is None:
            raise ImportError("colpali_engine n'est pas disponible")

        # DÃ©charger l'ancien modÃ¨le si nÃ©cessaire
        if self._model is not None:
            self._unload_model()

        try:
            logger.info(f"ðŸ“¦ Chargement du modÃ¨le {model_path} sur GPU...")
            start_time = time.time()

            # DÃ©terminer le device
            device = self._get_device()

            if device == "cpu":
                # Chargement CPU
                self._model = ColQwen2.from_pretrained(
                    model_path,
                    torch_dtype=torch.float32,
                    device_map="cpu",
                    low_cpu_mem_usage=True,
                ).eval()
            else:
                # Chargement GPU avec optimisations mÃ©moire
                self._model = ColQwen2.from_pretrained(
                    model_path,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    attn_implementation=("flash_attention_2" if is_flash_attn_2_available() else None),
                    low_cpu_mem_usage=True,
                    max_memory={0: "4.5GiB"},
                ).eval()

            # Charger le processeur
            self._processor = ColQwen2Processor.from_pretrained(model_path)
            self._model_path = model_path

            # Nettoyage du cache
            if device != "cpu" and torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()

            load_time = time.time() - start_time
            logger.info(f"âœ… ModÃ¨le chargÃ© en {load_time:.2f}s")

            # Afficher l'utilisation mÃ©moire
            if device != "cpu" and torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated(0) / 1024**3
                logger.info(f"ðŸ“Š MÃ©moire GPU utilisÃ©e: {allocated:.2f}GB")

        except Exception as e:
            logger.error(f"âŒ Erreur lors du chargement du modÃ¨le: {e}")
            self._model = None
            self._processor = None
            self._model_path = None
            raise

    def _unload_model(self) -> None:
        """DÃ©charge le modÃ¨le de la mÃ©moire GPU directement."""
        if self._model is None:
            return

        try:
            logger.info("ðŸ§¹ DÃ©chargement du modÃ¨le de la GPU...")

            # Supprimer les rÃ©fÃ©rences
            del self._model
            del self._processor
            self._model = None
            self._processor = None
            self._model_path = None

            # Forcer le garbage collection Python
            logger.info("   -> Garbage collection...")
            gc.collect()

            # Vider le cache CUDA de maniÃ¨re agressive
            if torch.cuda.is_available():
                logger.info("   -> Vidage agressif du cache CUDA...")

                # Plusieurs passes de nettoyage
                for _ in range(3):
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()

                # Essayer de rÃ©initialiser le contexte CUDA si possible
                try:
                    # RÃ©initialiser l'allocateur de mÃ©moire (PyTorch >= 1.10)
                    if hasattr(torch.cuda, "memory._set_allocator_settings"):
                        torch.cuda.memory._set_allocator_settings("")

                    # Forcer la libÃ©ration de toute la mÃ©moire rÃ©servÃ©e
                    torch.cuda.reset_peak_memory_stats()
                    torch.cuda.reset_accumulated_memory_stats()

                    # Si disponible, forcer un reset plus profond
                    if hasattr(torch.cuda, "cudart"):
                        torch.cuda.cudart().cudaDeviceReset()
                except Exception as e:
                    logger.debug(f"MÃ©thodes de reset CUDA non disponibles: {e}")

                # Afficher la mÃ©moire aprÃ¨s nettoyage
                after_allocated = torch.cuda.memory_allocated(0) / 1024**3
                after_reserved = torch.cuda.memory_reserved(0) / 1024**3

                logger.info("âœ… ModÃ¨le dÃ©chargÃ©:")
                logger.info(f"ðŸ“Š MÃ©moire GPU finale: AllouÃ©e={after_allocated:.2f}GB, RÃ©servÃ©e={after_reserved:.2f}GB")

                if after_reserved > 0.5:  # Plus de 500MB encore rÃ©servÃ©s
                    logger.warning(
                        f"âš ï¸ {after_reserved:.2f}GB de mÃ©moire encore rÃ©servÃ©e. "
                        "PyTorch garde la mÃ©moire en cache. DÃ©finissez PYTORCH_NO_CUDA_MEMORY_CACHING=1 "
                        "pour forcer la libÃ©ration complÃ¨te."
                    )
            else:
                logger.info("âœ… ModÃ¨le dÃ©chargÃ©")

        except Exception as e:
            logger.error(f"âŒ Erreur lors du dÃ©chargement: {e}")
            # MÃªme en cas d'erreur, s'assurer que les rÃ©fÃ©rences sont nulles
            self._model = None
            self._processor = None
            self._model_path = None

            # Tenter quand mÃªme un nettoyage basique
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    def force_unload(self) -> None:
        """Force le dÃ©chargement immÃ©diat du modÃ¨le, ignore les rÃ©fÃ©rences."""
        with self._load_lock:
            logger.info("âš ï¸ DÃ©chargement forcÃ© du modÃ¨le...")
            self._reference_count = 0
            self._unload_model()

    def _get_device(self) -> str:
        """DÃ©termine le meilleur device disponible.

        Returns:
            str: Le device Ã  utiliser ("cuda:0" ou "cpu").
        """
        if torch.cuda.is_available():
            gpu_props = torch.cuda.get_device_properties(0)
            gpu_mem = gpu_props.total_memory
            allocated_mem = torch.cuda.memory_allocated(0)
            free_mem = gpu_mem - allocated_mem

            if free_mem >= 4 * 1024 * 1024 * 1024:
                return "cuda:0"
            else:
                logger.warning("âš ï¸ MÃ©moire GPU insuffisante, utilisation du CPU")
                return "cpu"
        else:
            logger.info("â„¹ï¸ CUDA non disponible, utilisation du CPU")
            return "cpu"

    @property
    def is_loaded(self) -> bool:
        """VÃ©rifie si le modÃ¨le est chargÃ©.

        Returns:
            bool: True si le modÃ¨le est chargÃ© en mÃ©moire.
        """
        return self._model is not None

    def get_model_info(self) -> dict[str, Any]:
        """Retourne des informations sur le modÃ¨le.

        Returns:
            dict[str, Any]: Dictionnaire contenant les informations du modÃ¨le.
        """
        info = {
            "loaded": self.is_loaded,
            "model_path": self._model_path,
            "reference_count": self._reference_count,
            "last_used": (time.time() - self._last_used if self._last_used > 0 else None),
        }

        if self.is_loaded and torch.cuda.is_available():
            info["gpu_memory_allocated"] = f"{torch.cuda.memory_allocated(0) / 1024**3:.2f} GB"

        return info


class ColPaliModel:
    """Interface pour utiliser le modÃ¨le ColPali avec gestion automatique du cycle de vie."""

    def __init__(self, manager: ColPaliModelManager):
        """Initialise avec une rÃ©fÃ©rence au gestionnaire.

        Args:
            manager (ColPaliModelManager): Le gestionnaire de modÃ¨le Ã  utiliser.
        """
        self._manager = manager
        self.batch_size = 1

    def __enter__(self) -> "ColPaliModel":
        """Contexte manager pour acquisition automatique.

        Returns:
            ColPaliModel: L'instance elle-mÃªme pour le context manager.
        """
        return self

    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
        """LibÃ¨re automatiquement la rÃ©fÃ©rence.

        Args:
            exc_type: Type d'exception (si levÃ©e).
            exc_val: Valeur de l'exception (si levÃ©e).
            exc_tb: Traceback de l'exception (si levÃ©e).
        """
        self._manager.release()

    @property
    def device(self) -> str:
        """Retourne le device du modÃ¨le.

        Returns:
            str: Le device utilisÃ© par le modÃ¨le.
        """
        if self._manager._model is None:
            return "cpu"
        return str(self._manager._model.device)

    def generate_embeddings(self, images: Sequence[Image.Image | str]) -> list[list[float]]:
        """GÃ©nÃ¨re des embeddings pour une liste d'images.

        Args:
            images (Sequence[Image.Image | str]): Liste d'images ou de chemins vers les images.

        Returns:
            list[list[float]]: Liste des embeddings gÃ©nÃ©rÃ©s.

        Raises:
            RuntimeError: Si le modÃ¨le n'est pas chargÃ©.
        """
        if not images:
            return []

        if not self._manager.is_loaded:
            raise RuntimeError("Le modÃ¨le n'est pas chargÃ©. Utilisez ColPaliModelManager.acquire()")

        # Convertir en liste pour le traitement
        images_list = list(images)
        embeddings = []

        # Traiter par batch
        for i in range(0, len(images_list), self.batch_size):
            batch = images_list[i : i + self.batch_size]

            # Charger et prÃ©parer les images du batch
            batch_images = []
            for img in batch:
                if isinstance(img, str):
                    img = Image.open(img)
                if img.mode != "RGB":
                    img = img.convert("RGB")
                batch_images.append(img)

            try:
                with torch.no_grad():
                    # VÃ©rifier que le processeur et le modÃ¨le sont chargÃ©s
                    if self._manager._processor is None or self._manager._model is None:
                        raise RuntimeError("Le modÃ¨le ou le processeur n'est pas chargÃ©")

                    # Traiter les images du batch
                    batch_doc = self._manager._processor.process_images(batch_images)
                    batch_doc = {k: v.to(self._manager._model.device) for k, v in batch_doc.items()}

                    # GÃ©nÃ©rer les embeddings
                    batch_embeddings = self._manager._model(**batch_doc)

                    # Moyenne sur la dimension des patches pour chaque image
                    for j in range(batch_embeddings.shape[0]):
                        mean_embedding = torch.mean(batch_embeddings[j], dim=0).float().cpu().numpy()
                        embeddings.append(mean_embedding.tolist())

                    # LibÃ©rer la mÃ©moire GPU aprÃ¨s chaque batch
                    del batch_embeddings
                    del batch_doc

                    if self.device != "cpu" and torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        gc.collect()

            except torch.cuda.OutOfMemoryError:
                logger.error(f"âŒ MÃ©moire GPU insuffisante pour le batch {i//self.batch_size + 1}")
                if self.device != "cpu" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    gc.collect()
                # Ajouter des embeddings vides
                for _ in batch:
                    embeddings.append([0.0] * 128)

            except Exception as e:
                logger.error(f"âŒ Erreur lors de la gÃ©nÃ©ration de l'embedding: {e}")
                for _ in batch:
                    embeddings.append([0.0] * 128)

        return embeddings

    def generate_single_embedding(self, image: Image.Image | str) -> list[float]:
        """GÃ©nÃ¨re un embedding pour une seule image.

        Args:
            image (Image.Image | str): Image ou chemin vers l'image.

        Returns:
            list[float]: Embedding gÃ©nÃ©rÃ© pour l'image.
        """
        embeddings = self.generate_embeddings([image])
        return embeddings[0] if embeddings else [0.0] * 128

    def generate_query_embedding(self, query: str) -> list[float]:
        """GÃ©nÃ¨re un embedding pour une requÃªte textuelle.

        Args:
            query (str): RequÃªte textuelle Ã  traiter.

        Returns:
            list[float]: Embedding gÃ©nÃ©rÃ© pour la requÃªte.

        Raises:
            RuntimeError: Si le modÃ¨le n'est pas chargÃ©.
        """
        if not self._manager.is_loaded:
            raise RuntimeError("Le modÃ¨le n'est pas chargÃ©")

        try:
            with torch.no_grad():
                # VÃ©rifier que le processeur et le modÃ¨le sont chargÃ©s
                if self._manager._processor is None or self._manager._model is None:
                    raise RuntimeError("Le modÃ¨le ou le processeur n'est pas chargÃ©")

                batch_doc = self._manager._processor.process_queries([query]).to(self._manager._model.device)
                embeddings = self._manager._model(**batch_doc)

                # Moyenne sur la dimension des tokens
                mean_embedding = torch.mean(embeddings, dim=1).float().cpu().numpy()[0]

                # LibÃ©rer la mÃ©moire
                del embeddings
                del batch_doc

                if self.device != "cpu" and torch.cuda.is_available():
                    torch.cuda.empty_cache()

            return mean_embedding.tolist()

        except Exception as e:
            logger.error(f"âŒ Erreur lors de la gÃ©nÃ©ration de l'embedding de requÃªte: {e}")
            raise

    def batch_generate_embeddings(self, images: Sequence[Image.Image | str]) -> list[list[float]]:
        """Alias pour generate_embeddings.

        Args:
            images (Sequence[Image.Image | str]): Liste d'images ou de chemins vers les images.

        Returns:
            list[list[float]]: Liste des embeddings gÃ©nÃ©rÃ©s.
        """
        return self.generate_embeddings(images)

    def cleanup(self) -> None:
        """LibÃ¨re la rÃ©fÃ©rence au modÃ¨le."""
        self._manager.release()

    def get_model_info(self) -> dict[str, Any]:
        """Retourne des informations sur le modÃ¨le.

        Returns:
            dict[str, Any]: Dictionnaire contenant les informations du modÃ¨le.
        """
        info = self._manager.get_model_info()
        info["batch_size"] = self.batch_size
        info["embedding_dimension"] = 128
        return info


# Fonction utilitaire pour obtenir le gestionnaire singleton
def get_colpali_manager() -> ColPaliModelManager:
    """Retourne l'instance singleton du gestionnaire ColPali.

    Returns:
        ColPaliModelManager: L'instance unique du gestionnaire ColPali.
    """
    return ColPaliModelManager()
