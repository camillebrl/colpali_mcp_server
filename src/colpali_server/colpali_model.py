"""Mod√®le ColPali pour la g√©n√©ration d'embeddings d'images et de requ√™tes."""

import gc
import logging
import time
from collections.abc import Sequence
from threading import Lock
from typing import Any, Optional

import torch
from PIL import Image
from transformers.utils.import_utils import is_flash_attn_2_available

try:
    from colpali_engine.models import ColQwen2, ColQwen2Processor
except ImportError:
    ColQwen2 = None
    ColQwen2Processor = None


logger = logging.getLogger(__name__)


class ColPaliModelManager:
    """Gestionnaire singleton pour le mod√®le ColPali avec chargement/d√©chargement dynamique."""

    _instance: Optional["ColPaliModelManager"] = None
    _lock = Lock()

    def __new__(cls, *args: Any, **kwargs: Any) -> "ColPaliModelManager":
        """Cr√©e ou retourne l'instance singleton du gestionnaire ColPali.

        Args:
            *args: Arguments positionnels (ignor√©s).
            **kwargs: Arguments nomm√©s (ignor√©s).

        Returns:
            ColPaliModelManager: L'instance unique du gestionnaire.
        """
        if not cls._instance:
            with cls._lock:
                if not cls._instance:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self) -> None:
        """Initialise le gestionnaire ColPali (une seule fois pour le singleton)."""
        # Ne r√©initialiser que si c'est la premi√®re fois
        if not hasattr(self, "_initialized"):
            self._initialized = True
            self._model: ColQwen2 | None = None
            self._processor: ColQwen2Processor | None = None
            self._model_path: str | None = None
            self._last_used: float = 0
            self._load_lock = Lock()
            self._reference_count = 0
            self._unload_delay = 0  # D√©lai en secondes avant d√©chargement

    def acquire(self, model_path: str = "vidore/colqwen2-v1.0") -> "ColPaliModel":
        """Acquiert une r√©f√©rence au mod√®le et le charge si n√©cessaire.

        Args:
            model_path (str): Chemin vers le mod√®le ColPali √† charger.

        Returns:
            ColPaliModel: Instance du mod√®le pr√™te √† utiliser.
        """
        with self._load_lock:
            self._reference_count += 1
            self._last_used = time.time()

            # Charger le mod√®le si pas d√©j√† charg√© ou si le chemin a chang√©
            if self._model is None or self._model_path != model_path:
                self._load_model(model_path)

            return ColPaliModel(self)

    def release(self) -> None:
        """Lib√®re une r√©f√©rence au mod√®le."""
        with self._load_lock:
            self._reference_count = max(0, self._reference_count - 1)
            self._last_used = time.time()

            # Ne pas d√©charger imm√©diatement si d'autres r√©f√©rences existent
            if self._reference_count == 0:
                self._unload_model()
                return

            # Programmer le d√©chargement apr√®s le d√©lai
            # Note: Dans une vraie impl√©mentation, on utiliserait un timer thread
            # Pour simplifier, on laisse la responsabilit√© √† check_and_unload()

    def check_and_unload(self) -> None:
        """V√©rifie si le mod√®le doit √™tre d√©charg√© (appel√© p√©riodiquement)."""
        with self._load_lock:
            if (
                self._model is not None
                and self._reference_count == 0
                and time.time() - self._last_used > self._unload_delay
            ):
                self._unload_model()

    def _load_model(self, model_path: str) -> None:
        """Charge le mod√®le en m√©moire.

        Args:
            model_path (str): Chemin vers le mod√®le √† charger.

        Raises:
            ImportError: Si colpali_engine n'est pas disponible.
            Exception: Si le chargement √©choue.
        """
        if ColQwen2 is None or ColQwen2Processor is None:
            raise ImportError("colpali_engine n'est pas disponible")

        # D√©charger l'ancien mod√®le si n√©cessaire
        if self._model is not None:
            self._unload_model()

        try:
            logger.info(f"üì¶ Chargement du mod√®le {model_path} sur GPU...")
            start_time = time.time()

            # D√©terminer le device
            device = self._get_device()

            if device == "cpu":
                # Chargement CPU
                self._model = ColQwen2.from_pretrained(
                    model_path,
                    torch_dtype=torch.float32,
                    device_map="cpu",
                    low_cpu_mem_usage=True,
                ).eval()
            else:
                # Chargement GPU avec optimisations m√©moire
                self._model = ColQwen2.from_pretrained(
                    model_path,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    attn_implementation=("flash_attention_2" if is_flash_attn_2_available() else None),
                    low_cpu_mem_usage=True,
                    max_memory={0: "4.5GiB"},
                ).eval()

            # Charger le processeur
            self._processor = ColQwen2Processor.from_pretrained(model_path)
            self._model_path = model_path

            # Nettoyage du cache
            if device != "cpu" and torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()

            load_time = time.time() - start_time
            logger.info(f"‚úÖ Mod√®le charg√© en {load_time:.2f}s")

            # Afficher l'utilisation m√©moire
            if device != "cpu" and torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated(0) / 1024**3
                logger.info(f"üìä M√©moire GPU utilis√©e: {allocated:.2f}GB")

        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement du mod√®le: {e}")
            self._model = None
            self._processor = None
            self._model_path = None
            raise

    def _unload_model(self) -> None:
        """D√©charge le mod√®le de la m√©moire GPU directement."""
        if self._model is None:
            return

        try:
            logger.info("üßπ D√©chargement du mod√®le de la GPU...")

            # Supprimer les r√©f√©rences
            del self._model
            del self._processor
            self._model = None
            self._processor = None
            self._model_path = None

            # Forcer le garbage collection Python
            logger.info("   -> Garbage collection...")
            gc.collect()

            # Vider le cache CUDA de mani√®re agressive
            if torch.cuda.is_available():
                logger.info("   -> Vidage agressif du cache CUDA...")

                # Plusieurs passes de nettoyage
                for _ in range(3):
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()

                # Essayer de r√©initialiser le contexte CUDA si possible
                try:
                    # R√©initialiser l'allocateur de m√©moire (PyTorch >= 1.10)
                    if hasattr(torch.cuda, "memory._set_allocator_settings"):
                        torch.cuda.memory._set_allocator_settings("")

                    # Forcer la lib√©ration de toute la m√©moire r√©serv√©e
                    torch.cuda.reset_peak_memory_stats()
                    torch.cuda.reset_accumulated_memory_stats()

                    # Si disponible, forcer un reset plus profond
                    if hasattr(torch.cuda, "cudart"):
                        torch.cuda.cudart().cudaDeviceReset()
                except Exception as e:
                    logger.debug(f"M√©thodes de reset CUDA non disponibles: {e}")

                # Afficher la m√©moire apr√®s nettoyage
                after_allocated = torch.cuda.memory_allocated(0) / 1024**3
                after_reserved = torch.cuda.memory_reserved(0) / 1024**3

                logger.info("‚úÖ Mod√®le d√©charg√©:")
                logger.info(f"üìä M√©moire GPU finale: Allou√©e={after_allocated:.2f}GB, R√©serv√©e={after_reserved:.2f}GB")

                if after_reserved > 0.5:  # Plus de 500MB encore r√©serv√©s
                    logger.warning(
                        f"‚ö†Ô∏è {after_reserved:.2f}GB de m√©moire encore r√©serv√©e. "
                        "PyTorch garde la m√©moire en cache. D√©finissez PYTORCH_NO_CUDA_MEMORY_CACHING=1 "
                        "pour forcer la lib√©ration compl√®te."
                    )
            else:
                logger.info("‚úÖ Mod√®le d√©charg√©")

        except Exception as e:
            logger.error(f"‚ùå Erreur lors du d√©chargement: {e}")
            # M√™me en cas d'erreur, s'assurer que les r√©f√©rences sont nulles
            self._model = None
            self._processor = None
            self._model_path = None

            # Tenter quand m√™me un nettoyage basique
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    def force_unload(self) -> None:
        """Force le d√©chargement imm√©diat du mod√®le, ignore les r√©f√©rences."""
        with self._load_lock:
            logger.info("‚ö†Ô∏è D√©chargement forc√© du mod√®le...")
            self._reference_count = 0
            self._unload_model()

    def _get_device(self) -> str:
        """D√©termine le meilleur device disponible.

        Returns:
            str: Le device √† utiliser ("cuda:0" ou "cpu").
        """
        if torch.cuda.is_available():
            gpu_props = torch.cuda.get_device_properties(0)
            gpu_mem = gpu_props.total_memory
            allocated_mem = torch.cuda.memory_allocated(0)
            free_mem = gpu_mem - allocated_mem

            if free_mem >= 4 * 1024 * 1024 * 1024:
                return "cuda:0"
            else:
                logger.warning("‚ö†Ô∏è M√©moire GPU insuffisante, utilisation du CPU")
                return "cpu"
        else:
            logger.info("‚ÑπÔ∏è CUDA non disponible, utilisation du CPU")
            return "cpu"

    @property
    def is_loaded(self) -> bool:
        """V√©rifie si le mod√®le est charg√©.

        Returns:
            bool: True si le mod√®le est charg√© en m√©moire.
        """
        return self._model is not None

    def get_model_info(self) -> dict[str, Any]:
        """Retourne des informations sur le mod√®le.

        Returns:
            dict[str, Any]: Dictionnaire contenant les informations du mod√®le.
        """
        info = {
            "loaded": self.is_loaded,
            "model_path": self._model_path,
            "reference_count": self._reference_count,
            "last_used": (time.time() - self._last_used if self._last_used > 0 else None),
        }

        if self.is_loaded and torch.cuda.is_available():
            info["gpu_memory_allocated"] = f"{torch.cuda.memory_allocated(0) / 1024**3:.2f} GB"

        return info


class ColPaliModel:
    """Interface pour utiliser le mod√®le ColPali avec gestion automatique du cycle de vie."""

    def __init__(self, manager: ColPaliModelManager):
        """Initialise avec une r√©f√©rence au gestionnaire.

        Args:
            manager (ColPaliModelManager): Le gestionnaire de mod√®le √† utiliser.
        """
        self._manager = manager
        self.batch_size = 1

    def __enter__(self) -> "ColPaliModel":
        """Contexte manager pour acquisition automatique.

        Returns:
            ColPaliModel: L'instance elle-m√™me pour le context manager.
        """
        return self

    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
        """Lib√®re automatiquement la r√©f√©rence.

        Args:
            exc_type: Type d'exception (si lev√©e).
            exc_val: Valeur de l'exception (si lev√©e).
            exc_tb: Traceback de l'exception (si lev√©e).
        """
        self._manager.release()

    @property
    def device(self) -> str:
        """Retourne le device du mod√®le.

        Returns:
            str: Le device utilis√© par le mod√®le.
        """
        if self._manager._model is None:
            return "cpu"
        return str(self._manager._model.device)

    def generate_embeddings(self, images: Sequence[Image.Image | str]) -> list[list[float]]:
        """G√©n√®re des embeddings pour une liste d'images.

        Args:
            images (Sequence[Image.Image | str]): Liste d'images ou de chemins vers les images.

        Returns:
            list[list[float]]: Liste des embeddings g√©n√©r√©s.

        Raises:
            RuntimeError: Si le mod√®le n'est pas charg√©.
        """
        if not images:
            return []

        if not self._manager.is_loaded:
            raise RuntimeError("Le mod√®le n'est pas charg√©. Utilisez ColPaliModelManager.acquire()")

        # Convertir en liste pour le traitement
        images_list = list(images)
        embeddings = []

        # Traiter par batch
        for i in range(0, len(images_list), self.batch_size):
            batch = images_list[i : i + self.batch_size]

            # Charger et pr√©parer les images du batch
            batch_images = []
            for img in batch:
                if isinstance(img, str):
                    img = Image.open(img)
                if img.mode != "RGB":
                    img = img.convert("RGB")
                batch_images.append(img)

            try:
                with torch.no_grad():
                    # V√©rifier que le processeur et le mod√®le sont charg√©s
                    if self._manager._processor is None or self._manager._model is None:
                        raise RuntimeError("Le mod√®le ou le processeur n'est pas charg√©")

                    # Traiter les images du batch
                    batch_doc = self._manager._processor.process_images(batch_images)
                    batch_doc = {k: v.to(self._manager._model.device) for k, v in batch_doc.items()}

                    # G√©n√©rer les embeddings
                    batch_embeddings = self._manager._model(**batch_doc)

                    # Moyenne sur la dimension des patches pour chaque image
                    for j in range(batch_embeddings.shape[0]):
                        mean_embedding = torch.mean(batch_embeddings[j], dim=0).float().cpu().numpy()
                        embeddings.append(mean_embedding.tolist())

                    # Lib√©rer la m√©moire GPU apr√®s chaque batch
                    del batch_embeddings
                    del batch_doc

                    if self.device != "cpu" and torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        gc.collect()

            except torch.cuda.OutOfMemoryError:
                logger.error(f"‚ùå M√©moire GPU insuffisante pour le batch {i//self.batch_size + 1}")
                if self.device != "cpu" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    gc.collect()
                # Ajouter des embeddings vides
                for _ in batch:
                    embeddings.append([0.0] * 128)

            except Exception as e:
                logger.error(f"‚ùå Erreur lors de la g√©n√©ration de l'embedding: {e}")
                for _ in batch:
                    embeddings.append([0.0] * 128)

        return embeddings

    def generate_single_embedding(self, image: Image.Image | str) -> list[float]:
        """G√©n√®re un embedding pour une seule image.

        Args:
            image (Image.Image | str): Image ou chemin vers l'image.

        Returns:
            list[float]: Embedding g√©n√©r√© pour l'image.
        """
        embeddings = self.generate_embeddings([image])
        return embeddings[0] if embeddings else [0.0] * 128

    def generate_query_embedding(self, query: str) -> list[float]:
        """G√©n√®re un embedding pour une requ√™te textuelle.

        Args:
            query (str): Requ√™te textuelle √† traiter.

        Returns:
            list[float]: Embedding g√©n√©r√© pour la requ√™te.

        Raises:
            RuntimeError: Si le mod√®le n'est pas charg√©.
        """
        if not self._manager.is_loaded:
            raise RuntimeError("Le mod√®le n'est pas charg√©")

        try:
            with torch.no_grad():
                # V√©rifier que le processeur et le mod√®le sont charg√©s
                if self._manager._processor is None or self._manager._model is None:
                    raise RuntimeError("Le mod√®le ou le processeur n'est pas charg√©")

                batch_doc = self._manager._processor.process_queries([query]).to(self._manager._model.device)
                embeddings = self._manager._model(**batch_doc)

                # Moyenne sur la dimension des tokens
                mean_embedding = torch.mean(embeddings, dim=1).float().cpu().numpy()[0]

                # Lib√©rer la m√©moire
                del embeddings
                del batch_doc

                if self.device != "cpu" and torch.cuda.is_available():
                    torch.cuda.empty_cache()

            return mean_embedding.tolist()

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la g√©n√©ration de l'embedding de requ√™te: {e}")
            raise

    def batch_generate_embeddings(self, images: Sequence[Image.Image | str]) -> list[list[float]]:
        """Alias pour generate_embeddings.

        Args:
            images (Sequence[Image.Image | str]): Liste d'images ou de chemins vers les images.

        Returns:
            list[list[float]]: Liste des embeddings g√©n√©r√©s.
        """
        return self.generate_embeddings(images)

    def cleanup(self) -> None:
        """Lib√®re la r√©f√©rence au mod√®le."""
        self._manager.release()

    def get_model_info(self) -> dict[str, Any]:
        """Retourne des informations sur le mod√®le.

        Returns:
            dict[str, Any]: Dictionnaire contenant les informations du mod√®le.
        """
        info = self._manager.get_model_info()
        info["batch_size"] = self.batch_size
        info["embedding_dimension"] = 128
        return info


# Fonction utilitaire pour obtenir le gestionnaire singleton
def get_colpali_manager() -> ColPaliModelManager:
    """Retourne l'instance singleton du gestionnaire ColPali.

    Returns:
        ColPaliModelManager: L'instance unique du gestionnaire ColPali.
    """
    return ColPaliModelManager()
